{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993a4beb",
   "metadata": {},
   "source": [
    "## Importing Depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Input, Conv2D, Reshape, Bidirectional, LSTM, \n",
    "                                     Dropout, Dense, TimeDistributed, GlobalAveragePooling1D, \n",
    "                                     Attention, concatenate, RNN, LSTMCell) # <-- Add RNN and LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, Conv2D, Reshape, Bidirectional, LSTM, Dropout, Attention, concatenate, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    _HAVE_SKLEARN = True\n",
    "except Exception:\n",
    "    _HAVE_SKLEARN = False\n",
    "\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'happy', 'sad', 'surprised', 'satisfied',\n",
    "    'protected', 'frightened', 'angry', 'unconcerned'\n",
    "]\n",
    "\n",
    "\n",
    "FOLDER_TO_CLASS = {\n",
    "    'Happy': 'happy',\n",
    "    'Sad': 'sad', \n",
    "    'Surprise': 'surprised',  \n",
    "    'Satisfied': 'satisfied',\n",
    "    'Protected': 'protected',\n",
    "    'Frightened': 'frightened',\n",
    "    'Angry': 'angry',\n",
    "    'Unconcerned': 'unconcerned'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebccb56",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_numeric_csv(filepath: str) -> np.ndarray:\n",
    "    \n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    if numeric_df.shape[1] == 0:\n",
    "        numeric_df = df.apply(pd.to_numeric, errors='coerce').dropna(axis=1, how='all')\n",
    "    numeric_df = numeric_df.dropna(how='all')\n",
    "    arr = numeric_df.values.astype(float)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr[:, None]\n",
    "    return arr\n",
    "\n",
    "def _ensure_shape_and_resample(raw: np.ndarray, time_steps: int, channels: int, features: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ensure the data has the correct shape by padding or truncating.\n",
    "    Reshapes the data to (time_steps, channels, features).\n",
    "    \"\"\"\n",
    "    r, c = raw.shape\n",
    "    if r == time_steps and c == channels * features:\n",
    "        return raw.reshape((time_steps, channels, features))\n",
    "    flat = raw.reshape((-1,))\n",
    "    needed = time_steps * channels * features\n",
    "    if flat.size >= needed:\n",
    "        flat = flat[:needed]\n",
    "    else:\n",
    "        pad = np.zeros(needed - flat.size, dtype=flat.dtype)\n",
    "        flat = np.concatenate([flat, pad], axis=0)\n",
    "    return flat.reshape((time_steps, channels, features))\n",
    "\n",
    "def _normalize_per_sample(sample: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize each sample by subtracting mean and dividing by standard deviation.\n",
    "    Handles cases with very small standard deviation.\n",
    "    \"\"\"\n",
    "    mean = sample.mean(axis=0, keepdims=True)\n",
    "    std = sample.std(axis=0, keepdims=True)\n",
    "    std[std < 1e-8] = 1.0\n",
    "    return (sample - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba288e",
   "metadata": {},
   "source": [
    "## Denoising the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae08f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def wavelet_denoise(data, wavelet='db4', level=4):\n",
    "    \"\"\"\n",
    "    Applies wavelet denoising to a signal.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): The input signal.\n",
    "        wavelet (str): The type of wavelet to use.\n",
    "        level (int): The level of decomposition.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The denoised signal.\n",
    "    \"\"\"\n",
    "    # Decompose to get the wavelet coefficients\n",
    "    coeffs = pywt.wavedec(data, wavelet, level=level)\n",
    "\n",
    "    # Calculate threshold on the last level of detail coefficients\n",
    "    # Using Median Absolute Deviation (MAD) for a robust noise estimate\n",
    "    sigma = np.median(np.abs(coeffs[-1] - np.median(coeffs[-1]))) / 0.6745\n",
    "    \n",
    "    # Universal Threshold from Donoho & Johnstone\n",
    "    threshold = sigma * np.sqrt(2 * np.log(len(data)))\n",
    "\n",
    "    # Apply soft thresholding to the detail coefficients\n",
    "    new_coeffs = coeffs.copy()\n",
    "    for i in range(1, len(coeffs)):\n",
    "        new_coeffs[i] = pywt.threshold(coeffs[i], value=threshold, mode='soft')\n",
    "\n",
    "    # Reconstruct the signal from the thresholded coefficients\n",
    "    reconstructed_signal = pywt.waverec(new_coeffs, wavelet)\n",
    "    \n",
    "    # The reconstructed signal might be slightly shorter/longer, trim to original length\n",
    "    return reconstructed_signal[:len(data)]\n",
    "\n",
    "def process_eeg_dataset(dataset_path, output_path):\n",
    "    \"\"\"\n",
    "    Loads, denoises, and saves the entire EEG dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): The path to the root of the dataset directory.\n",
    "        output_path (str): The path where the denoised data will be saved.\n",
    "    \"\"\"\n",
    "    # These are the 14 main EEG channels based on the file structure\n",
    "    eeg_channels = [\n",
    "        'EEG.AF3', 'EEG.F7', 'EEG.F3', 'EEG.FC5', 'EEG.T7', 'EEG.P7', 'EEG.O1',\n",
    "        'EEG.O2', 'EEG.P8', 'EEG.T8', 'EEG.FC6', 'EEG.F4', 'EEG.F8', 'EEG.AF4'\n",
    "    ]\n",
    "\n",
    "    print(f\"Starting processing for dataset at: {dataset_path}\")\n",
    "\n",
    "    # Walk through the directory tree\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"  - Processing file: {file_path}\")\n",
    "\n",
    "                try:\n",
    "                    # Load the data, skipping the metadata in the first row\n",
    "                    df = pd.read_csv(file_path, skiprows=1)\n",
    "                    \n",
    "                    # Create a new dataframe for the denoised data\n",
    "                    denoised_df = pd.DataFrame()\n",
    "                    \n",
    "                    # Keep timestamp column if it exists\n",
    "                    if 'Timestamp' in df.columns:\n",
    "                        denoised_df['Timestamp'] = df['Timestamp']\n",
    "\n",
    "                    for channel in eeg_channels:\n",
    "                        if channel in df.columns:\n",
    "                            # Get the signal and handle any missing values\n",
    "                            signal = df[channel].dropna().values\n",
    "                            \n",
    "                            # --- CRITICAL CHECK ---\n",
    "                            # Only denoise if the signal actually has variation\n",
    "                            if np.var(signal) > 0:\n",
    "                                denoised_signal = wavelet_denoise(signal)\n",
    "                                denoised_df[channel] = pd.Series(denoised_signal)\n",
    "                            else:\n",
    "                                # If signal is flat, just copy it over\n",
    "                                denoised_df[channel] = pd.Series(signal)\n",
    "                        else:\n",
    "                            print(f\"    - Warning: Channel {channel} not found in {file}\")\n",
    "                    \n",
    "                    # Create the corresponding output directory structure\n",
    "                    relative_path = os.path.relpath(root, dataset_path)\n",
    "                    output_dir = os.path.join(output_path, relative_path)\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                    # Save the denoised dataframe to a new CSV file\n",
    "                    output_file_path = os.path.join(output_dir, file)\n",
    "                    denoised_df.to_csv(output_file_path, index=False)\n",
    "                    print(f\"    - Saved denoised file to: {output_file_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    - Error processing file {file_path}: {e}\")\n",
    "\n",
    "    print(\"\\nDataset processing complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f8d0d",
   "metadata": {},
   "source": [
    "## Updated Dataset Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eeg_dataset(\n",
    "    data_dir: str,\n",
    "    time_steps: int = 128,\n",
    "    channels: int = 55,\n",
    "    features: int = 64,\n",
    "    stressed_classes: Optional[List[str]] = None,\n",
    "    test_size: float = 0.15,\n",
    "    val_size: float = 0.15,\n",
    "    random_state: int = 42,\n",
    "    batch_size: int = 4\n",
    ") -> Tuple[Dict[str, tf.data.Dataset], Dict]:\n",
    "    \"\"\"\n",
    "    Load EEG dataset from structured folders with class subdirectories.\n",
    "    Returns datasets and metadata including class weights.\n",
    "    \"\"\"\n",
    "    if stressed_classes is None:\n",
    "        stressed_classes = ['frightened', 'angry']\n",
    "\n",
    "    # Find all CSV files in class subdirectories (excluding Baseline)\n",
    "    files, labels = [], []\n",
    "    for folder_name, class_name in FOLDER_TO_CLASS.items():\n",
    "        cls_folder = os.path.join(data_dir, folder_name)\n",
    "        found = glob.glob(os.path.join(cls_folder, \"*.csv\"))\n",
    "        for f in found:\n",
    "            files.append(f)\n",
    "            labels.append(class_name)\n",
    "\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(\"No CSV files found. Make sure your directory contains class folders with CSVs.\")\n",
    "\n",
    "    # Process each file\n",
    "    X_list, y_multi_idx, y_binary = [], [], []\n",
    "    for fpath, cls in zip(files, labels):\n",
    "        raw = _read_numeric_csv(fpath)\n",
    "\n",
    "        # =========================================================\n",
    "        # NEW STEP: Apply wavelet denoising after reading the data\n",
    "        raw = _denoise_with_wavelet(raw)\n",
    "        # =========================================================\n",
    "\n",
    "        sample = _ensure_shape_and_resample(raw, time_steps, channels, features)\n",
    "        sample = _normalize_per_sample(sample)\n",
    "        X_list.append(sample.astype(np.float32))\n",
    "        idx = CLASS_NAMES.index(cls)\n",
    "        y_multi_idx.append(idx)\n",
    "        y_binary.append(1 if cls in stressed_classes else 0)\n",
    "\n",
    "    # Convert to arrays\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y_multi_idx = np.array(y_multi_idx, dtype=np.int32)\n",
    "    y_binary = np.array(y_binary, dtype=np.float32)\n",
    "    y_multi_onehot = tf.keras.utils.to_categorical(y_multi_idx, num_classes=len(CLASS_NAMES)).astype(np.float32)\n",
    "\n",
    "    # ... (the rest of the function remains exactly the same) ...\n",
    "\n",
    "    # Calculate class weights for imbalanced datasets\n",
    "    class_weights = None\n",
    "    if _HAVE_SKLEARN:\n",
    "        weights = compute_class_weight('balanced', classes=np.arange(len(CLASS_NAMES)), y=y_multi_idx)\n",
    "        class_weights = {i: float(w) for i, w in enumerate(weights)}\n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    if _HAVE_SKLEARN:\n",
    "        test_val = test_size + val_size\n",
    "        train_idx, temp_idx, _, y_temp = train_test_split(\n",
    "            np.arange(len(X)), y_multi_idx, test_size=test_val,\n",
    "            random_state=random_state, stratify=y_multi_idx\n",
    "        )\n",
    "        if val_size > 0:\n",
    "            val_idx, test_idx, _, _ = train_test_split(\n",
    "                temp_idx, y_temp, test_size=(test_size / test_val),\n",
    "                random_state=random_state, stratify=y_temp\n",
    "            )\n",
    "        else:\n",
    "            val_idx, test_idx = [], temp_idx\n",
    "    else:\n",
    "        N = len(X)\n",
    "        idxs = np.arange(N)\n",
    "        np.random.seed(random_state)\n",
    "        np.random.shuffle(idxs)\n",
    "        n_test = int(N * test_size)\n",
    "        n_val = int(N * val_size)\n",
    "        test_idx = idxs[:n_test]\n",
    "        val_idx = idxs[n_test:n_test+n_val]\n",
    "        train_idx = idxs[n_test+n_val:]\n",
    "\n",
    "    # Create TensorFlow datasets\n",
    "    def make_ds(indices, weights_dict):\n",
    "        x = X[indices]\n",
    "        y_dict = {\n",
    "            'stressed_not_stressed_output': y_binary[indices],\n",
    "            'emotion_class_output': y_multi_onehot[indices]\n",
    "        }\n",
    "        emotion_labels = y_multi_idx[indices]\n",
    "        sample_weights_emotion = np.array([weights_dict[label] for label in emotion_labels])\n",
    "        sample_weights_stress = np.ones_like(y_binary[indices])\n",
    "        sample_weights = {\n",
    "            'stressed_not_stressed_output': sample_weights_stress,\n",
    "            'emotion_class_output': sample_weights_emotion\n",
    "        }\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x, y_dict, sample_weights))\n",
    "        ds = ds.shuffle(buffer_size=len(indices), seed=random_state).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    datasets = {\n",
    "        'train': make_ds(train_idx, class_weights),\n",
    "        'val': make_ds(val_idx, class_weights) if len(val_idx) > 0 else None,\n",
    "        'test': make_ds(test_idx, class_weights) if len(test_idx) > 0 else None\n",
    "    }\n",
    "\n",
    "    class_counts = {cls: 0 for cls in CLASS_NAMES}\n",
    "    for cls in labels:\n",
    "        class_counts[cls] += 1\n",
    "\n",
    "    meta = {\n",
    "        'class_to_index': {c: i for i, c in enumerate(CLASS_NAMES)},\n",
    "        'index_to_class': {i: c for i, c in enumerate(CLASS_NAMES)},\n",
    "        'counts': class_counts,\n",
    "        'total_samples': len(X),\n",
    "        'class_weights': class_weights\n",
    "    }\n",
    "\n",
    "    return datasets, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59bb90",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6063af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eeg_model(input_shape):\n",
    "    \"\"\"\n",
    "    Create a 'lighter' and stable multi-output model to consume less memory.\n",
    "    It includes the 'hard_sigmoid' fix to prevent CudnnRNN errors.\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Lambda(lambda z: tf.expand_dims(z, axis=-1))(input_layer)\n",
    "\n",
    "    # Reduced filters in Conv2D layers\n",
    "    x = TimeDistributed(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))(x)\n",
    "    x = TimeDistributed(Conv2D(filters=8, kernel_size=(3, 3), activation='relu', padding='same'))(x)\n",
    "\n",
    "    x = Reshape((input_shape[0], -1))(x)\n",
    "\n",
    "    # Reduced units in LSTM layers and the required 'hard_sigmoid' fix\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Bidirectional(LSTM(16, return_sequences=True))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Attention and Output layers\n",
    "    attention_output = Attention()([x, x])\n",
    "    x = concatenate([x, attention_output], axis=-1)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    binary_head = Dense(32, activation='relu')(x)\n",
    "    binary_head = Dropout(0.25)(binary_head)\n",
    "    binary_head_output = Dense(1, activation='sigmoid', name='stressed_not_stressed_output')(binary_head)\n",
    "\n",
    "    multiclass_head = Dense(32, activation='relu')(x)\n",
    "    multiclass_head = Dropout(0.25)(multiclass_head)\n",
    "    multiclass_head_output = Dense(len(CLASS_NAMES), activation='softmax', name='emotion_class_output')(multiclass_head)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=[binary_head_output, multiclass_head_output])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d9d93",
   "metadata": {},
   "source": [
    "## Model Instantiation and Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape parameters\n",
    "INPUT_TIME_STEPS = 128\n",
    "INPUT_CHANNELS = 55\n",
    "INPUT_FEATURES = 64\n",
    "INPUT_3D_SHAPE = (INPUT_TIME_STEPS, INPUT_CHANNELS, INPUT_FEATURES)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.0001, amsgrad=True)  # <-- Add amssgrad=True)\n",
    "\n",
    "# Create the model\n",
    "model = create_eeg_model(INPUT_3D_SHAPE)\n",
    "\n",
    "# Compile the model with appropriate loss functions and metrics for each output\n",
    "model.compile(optimizer=optimizer, run_eagerly=True,\n",
    "              loss={'stressed_not_stressed_output': 'binary_crossentropy',\n",
    "                    'emotion_class_output': 'categorical_crossentropy'},\n",
    "              metrics={'stressed_not_stressed_output': ['accuracy'],\n",
    "                       'emotion_class_output': ['accuracy']},\n",
    "              weighted_metrics=[]) # <-- Add this line\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156d9b6",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d75b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the functions with your dataset\n",
    "# Note: Replace with your actual dataset path\n",
    "dataset_path = \"/media/kd/New Volume/Github/EEG-Emotion-Detection/dataset\"\n",
    "\n",
    "# Load the dataset\n",
    "datasets, meta = load_eeg_dataset(dataset_path)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Total samples: {meta['total_samples']}\")\n",
    "print(\"Samples per class:\")\n",
    "for cls, count in meta['counts'].items():\n",
    "    print(f\"  {cls}: {count}\")\n",
    "\n",
    "print(\"\\nClass weights:\")\n",
    "for idx, weight in meta['class_weights'].items():\n",
    "    print(f\"  {meta['index_to_class'][idx]}: {weight:.2f}\")\n",
    "\n",
    "# The returned datasets dictionary contains:\n",
    "# - datasets['train']: Training dataset\n",
    "# - datasets['val']: Validation dataset (if val_size > 0)\n",
    "# - datasets['test']: Test dataset\n",
    "\n",
    "# Example training (uncomment when ready to train):\n",
    "# history = model.fit(\n",
    "#     datasets['train'],\n",
    "#     validation_data=datasets['val'],\n",
    "#     epochs=50,\n",
    "#     class_weight={'emotion_class_output': meta['class_weights']}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999d165",
   "metadata": {},
   "source": [
    "## Additional Utility Function for Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb63359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Explore the dataset structure and count files in each class folder.\n",
    "    \"\"\"\n",
    "    print(\"Dataset structure exploration:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_files = 0\n",
    "    for folder_name, class_name in FOLDER_TO_CLASS.items():\n",
    "        cls_folder = os.path.join(data_dir, folder_name)\n",
    "        found = glob.glob(os.path.join(cls_folder, \"*.csv\"))\n",
    "        print(f\"{folder_name} ({class_name}): {len(found)} files\")\n",
    "        total_files += len(found)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total files (excluding Baseline): {total_files}\")\n",
    "    \n",
    "    # Check Baseline folder separately\n",
    "    baseline_folder = os.path.join(data_dir, \"Baseline\")\n",
    "    baseline_files = glob.glob(os.path.join(baseline_folder, \"*.csv\"))\n",
    "    print(f\"Baseline files (excluded): {len(baseline_files)}\")\n",
    "    \n",
    "    return total_files\n",
    "\n",
    "# Explore your dataset\n",
    "total_files = explore_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf188cc6",
   "metadata": {},
   "source": [
    "## Define Callbacks for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Create a timestamp for model versioning\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # Save the best model during training\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'models/best_model_{timestamp}.h5',\n",
    "        monitor='val_emotion_class_output_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Stop training if no improvement after 15 epochs\n",
    "    EarlyStopping(\n",
    "        monitor='val_emotion_class_output_accuracy',\n",
    "        patience=100,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when validation accuracy plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_emotion_class_output_accuracy',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Log training progress for TensorBoard\n",
    "    TensorBoard(\n",
    "        log_dir=f'logs/fit/{timestamp}',\n",
    "        histogram_freq=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d32d70",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd31d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE\n",
    "history = model.fit(\n",
    "    datasets['train'],\n",
    "    epochs=100,\n",
    "    validation_data=datasets['val'],\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce83da",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "if datasets['test'] is not None:\n",
    "    test_results = model.evaluate(datasets['test'], verbose=1)\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Stress Detection Loss: {test_results[1]:.4f}\")\n",
    "    print(f\"Stress Detection Accuracy: {test_results[3]:.4f}\")\n",
    "    print(f\"Emotion Classification Loss: {test_results[2]:.4f}\")\n",
    "    print(f\"Emotion Classification Accuracy: {test_results[4]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5305243a",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378585be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Stress detection loss\n",
    "    axes[0, 0].plot(history.history['stressed_not_stressed_output_loss'], label='Training')\n",
    "    axes[0, 0].plot(history.history['val_stressed_not_stressed_output_loss'], label='Validation')\n",
    "    axes[0, 0].set_title('Stress Detection Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Stress detection accuracy\n",
    "    axes[0, 1].plot(history.history['stressed_not_stressed_output_accuracy'], label='Training')\n",
    "    axes[0, 1].plot(history.history['val_stressed_not_stressed_output_accuracy'], label='Validation')\n",
    "    axes[0, 1].set_title('Stress Detection Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Emotion classification loss\n",
    "    axes[1, 0].plot(history.history['emotion_class_output_loss'], label='Training')\n",
    "    axes[1, 0].plot(history.history['val_emotion_class_output_loss'], label='Validation')\n",
    "    axes[1, 0].set_title('Emotion Classification Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Emotion classification accuracy\n",
    "    axes[1, 1].plot(history.history['emotion_class_output_accuracy'], label='Training')\n",
    "    axes[1, 1].plot(history.history['val_emotion_class_output_accuracy'], label='Validation')\n",
    "    axes[1, 1].set_title('Emotion Classification Accuracy')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd48f26",
   "metadata": {},
   "source": [
    "## Make Predictions and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def analyze_predictions(model, dataset):\n",
    "    \"\"\"\n",
    "    Generates and displays classification reports and a confusion matrix.\n",
    "    This version correctly handles datasets that yield (features, labels, weights).\n",
    "    \"\"\"\n",
    "    # Get predictions for the entire dataset at once\n",
    "    predictions = model.predict(dataset)\n",
    "    \n",
    "    # --- THIS IS THE CORRECTED PART ---\n",
    "    # Get true labels by iterating through the batched dataset\n",
    "    y_true_stress = []\n",
    "    y_true_emotion = []\n",
    "    # The loop now correctly unpacks three items: features, labels, and weights.\n",
    "    # We ignore the features (x) and weights (_) since they are not needed here.\n",
    "    for _, y, _ in dataset:\n",
    "        y_true_stress.extend(y['stressed_not_stressed_output'].numpy())\n",
    "        y_true_emotion.extend(np.argmax(y['emotion_class_output'].numpy(), axis=1))\n",
    "    \n",
    "    y_true_stress = np.array(y_true_stress)\n",
    "    y_true_emotion = np.array(y_true_emotion)\n",
    "    \n",
    "    # Process predictions\n",
    "    y_pred_stress = (predictions[0] > 0.5).astype(int).flatten()\n",
    "    y_pred_emotion = np.argmax(predictions[1], axis=1)\n",
    "    \n",
    "    # --- The rest of the function remains the same ---\n",
    "    # Print classification reports\n",
    "    print(\"Stress Detection Report:\")\n",
    "    print(classification_report(y_true_stress, y_pred_stress, \n",
    "                                target_names=['Not Stressed', 'Stressed']))\n",
    "    \n",
    "    print(\"\\nEmotion Classification Report:\")\n",
    "    print(classification_report(y_true_emotion, y_pred_emotion, \n",
    "                                target_names=CLASS_NAMES))\n",
    "    \n",
    "    # Plot confusion matrix for emotions\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true_emotion, y_pred_emotion)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "    plt.title('Emotion Classification Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true_stress, y_pred_stress, y_true_emotion, y_pred_emotion\n",
    "\n",
    "# Now, when you call this function, it will work correctly\n",
    "if datasets['test'] is not None:\n",
    "    y_true_stress, y_pred_stress, y_true_emotion, y_pred_emotion = analyze_predictions(model, datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df45ef",
   "metadata": {},
   "source": [
    "## Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model\n",
    "model.save(f'models/final_model_{timestamp}.h5')\n",
    "print(f\"Model saved as models/final_model_{timestamp}.h5\")\n",
    "\n",
    "# Save the metadata for later use\n",
    "import json\n",
    "with open(f'models/model_metadata_{timestamp}.json', 'w') as f:\n",
    "    json.dump(meta, f)\n",
    "print(f\"Metadata saved as models/model_metadata_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1379d78d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
