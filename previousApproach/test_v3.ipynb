{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef35ed9a",
   "metadata": {},
   "source": [
    "### Imports & GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e18619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 05:56:16.069080: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory growth configured for 1 device(s).\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 0. Importing Dependencies\n",
    "# ==============================================================================\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pywt\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# Keras and TensorFlow Layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, MaxPooling1D, Bidirectional, LSTM,\n",
    "    Dropout, Dense, concatenate, Layer, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# Scikit-learn for data splitting and class weights\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    _HAVE_SKLEARN = True\n",
    "except ImportError:\n",
    "    _HAVE_SKLEARN = False\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. GPU Memory Configuration\n",
    "# ==============================================================================\n",
    "# Configure TensorFlow to grow GPU memory usage as needed.\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU memory growth configured for {len(gpus)} device(s).\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe9bd2",
   "metadata": {},
   "source": [
    "### Global Constants & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef9fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. Configuration and Constants\n",
    "# ==============================================================================\n",
    "CLASS_NAMES = [\n",
    "    'happy', 'sad', 'surprised', 'satisfied',\n",
    "    'protected', 'frightened', 'angry', 'unconcerned'\n",
    "]\n",
    "\n",
    "FOLDER_TO_CLASS = {\n",
    "    'Happy': 'happy',\n",
    "    'Sad': 'sad',\n",
    "    'Surprise': 'surprised',\n",
    "    'Satisfied': 'satisfied',\n",
    "    'Protected': 'protected',\n",
    "    'Frightened': 'frightened',\n",
    "    'Angry': 'angry',\n",
    "    'Unconcerned': 'unconcerned'\n",
    "}\n",
    "\n",
    "# We will focus on the 14 core EEG channels for a more robust signal\n",
    "EEG_CHANNELS = [\n",
    "    'EEG.AF3', 'EEG.F7', 'EEG.F3', 'EEG.FC5', 'EEG.T7', 'EEG.P7', 'EEG.O1',\n",
    "    'EEG.O2', 'EEG.P8', 'EEG.T8', 'EEG.FC6', 'EEG.F4', 'EEG.F8', 'EEG.AF4'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f563c",
   "metadata": {},
   "source": [
    "### Data Preprocessing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af271e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. Data Loading and Preprocessing Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def wavelet_denoise(data, wavelet='db4', level=4):\n",
    "    \"\"\"Applies wavelet denoising to a 1D signal.\"\"\"\n",
    "    coeffs = pywt.wavedec(data, wavelet, level=level)\n",
    "    sigma = np.median(np.abs(coeffs[-1] - np.median(coeffs[-1]))) / 0.6745\n",
    "    threshold = sigma * np.sqrt(2 * np.log(len(data)))\n",
    "    new_coeffs = coeffs.copy()\n",
    "    for i in range(1, len(coeffs)):\n",
    "        new_coeffs[i] = pywt.threshold(coeffs[i], value=threshold, mode='soft')\n",
    "    reconstructed_signal = pywt.waverec(new_coeffs, wavelet)\n",
    "    return reconstructed_signal[:len(data)]\n",
    "\n",
    "def _read_and_denoise_csv(filepath: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads a CSV file, applies wavelet denoising to the specified EEG channels,\n",
    "    and returns a NumPy array containing only those channels.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, skiprows=1, low_memory=False)\n",
    "    denoised_data = {}\n",
    "\n",
    "    for channel in EEG_CHANNELS:\n",
    "        if channel in df.columns:\n",
    "            signal = df[channel].dropna().values\n",
    "            if np.var(signal) > 0:\n",
    "                denoised_signal = wavelet_denoise(signal)\n",
    "                denoised_data[channel] = denoised_signal\n",
    "            else:\n",
    "                denoised_data[channel] = signal\n",
    "    \n",
    "    max_len = max(len(v) for v in denoised_data.values()) if denoised_data else 0\n",
    "    for channel in denoised_data:\n",
    "        if len(denoised_data[channel]) < max_len:\n",
    "            padding = np.zeros(max_len - len(denoised_data[channel]))\n",
    "            denoised_data[channel] = np.concatenate([denoised_data[channel], padding])\n",
    "            \n",
    "    arr = pd.DataFrame(denoised_data).values.astype(np.float32)\n",
    "    return arr\n",
    "\n",
    "def _ensure_shape_and_pad(raw: np.ndarray, time_steps: int, channels: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ensure the data has the correct 2D shape (time_steps, channels) by padding or truncating.\n",
    "    \"\"\"\n",
    "    if raw.shape[0] > time_steps:\n",
    "        return raw[:time_steps, :]\n",
    "    elif raw.shape[0] < time_steps:\n",
    "        padding = np.zeros((time_steps - raw.shape[0], channels), dtype=raw.dtype)\n",
    "        return np.concatenate([raw, padding], axis=0)\n",
    "    return raw\n",
    "\n",
    "def _normalize_per_sample(sample: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize each sample by subtracting mean and dividing by standard deviation.\"\"\"\n",
    "    mean = sample.mean(axis=0, keepdims=True)\n",
    "    std = sample.std(axis=0, keepdims=True)\n",
    "    std[std < 1e-8] = 1.0\n",
    "    return (sample - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c5b02",
   "metadata": {},
   "source": [
    "### Main Dataset Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6286a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. Main Dataset Loading Function\n",
    "# ==============================================================================\n",
    "\n",
    "def load_eeg_dataset(\n",
    "    data_dir: str, time_steps: int, channels: int,\n",
    "    stressed_classes: Optional[List[str]] = None, test_size: float = 0.15,\n",
    "    val_size: float = 0.15, random_state: int = 42, batch_size: int = 4\n",
    ") -> Tuple[Dict[str, tf.data.Dataset], Dict]:\n",
    "    \"\"\"Load EEG dataset with sample weights included directly for the Conv1D model.\"\"\"\n",
    "    if not _HAVE_SKLEARN:\n",
    "        raise ImportError(\"Scikit-learn is required. Please run 'pip install scikit-learn'.\")\n",
    "\n",
    "    if stressed_classes is None: stressed_classes = ['frightened', 'angry']\n",
    "    \n",
    "    files, labels = [], []\n",
    "    for folder, cls in FOLDER_TO_CLASS.items():\n",
    "        cls_folder = os.path.join(data_dir, folder)\n",
    "        if os.path.isdir(cls_folder):\n",
    "            found = glob.glob(os.path.join(cls_folder, \"*.csv\"))\n",
    "            files.extend(found)\n",
    "            labels.extend([cls] * len(found))\n",
    "\n",
    "    if not files: raise ValueError(f\"No CSV files found in subdirectories of {data_dir}.\")\n",
    "\n",
    "    X_list, y_multi_idx, y_binary = [], [], []\n",
    "    for fpath, cls in zip(files, labels):\n",
    "        raw = _read_and_denoise_csv(fpath)\n",
    "        if raw.shape[1] != channels:\n",
    "            temp = np.zeros((raw.shape[0], channels), dtype=raw.dtype)\n",
    "            min_cols = min(raw.shape[1], channels)\n",
    "            temp[:, :min_cols] = raw[:, :min_cols]\n",
    "            raw = temp\n",
    "        sample = _ensure_shape_and_pad(raw, time_steps, channels)\n",
    "        sample = _normalize_per_sample(sample)\n",
    "        X_list.append(sample.astype(np.float32))\n",
    "        y_multi_idx.append(CLASS_NAMES.index(cls))\n",
    "        y_binary.append(1 if cls in stressed_classes else 0)\n",
    "\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y_multi_idx = np.array(y_multi_idx, dtype=np.int32)\n",
    "    y_binary = np.array(y_binary, dtype=np.float32)\n",
    "    y_multi_onehot = tf.keras.utils.to_categorical(y_multi_idx, num_classes=len(CLASS_NAMES))\n",
    "    \n",
    "    weights = compute_class_weight('balanced', classes=np.arange(len(CLASS_NAMES)), y=y_multi_idx)\n",
    "    class_weights = {i: float(w) for i, w in enumerate(weights)}\n",
    "    \n",
    "    emotion_sample_weights = np.array([class_weights[label] for label in y_multi_idx], dtype=np.float32)\n",
    "    stress_sample_weights = np.ones_like(y_binary, dtype=np.float32)\n",
    "    \n",
    "    indices = np.arange(len(X))\n",
    "    train_indices, temp_indices = train_test_split(indices, test_size=(test_size + val_size), random_state=random_state, stratify=y_multi_idx)\n",
    "    val_indices, test_indices = train_test_split(temp_indices, test_size=(test_size / (test_size + val_size)), random_state=random_state, stratify=y_multi_idx[temp_indices])\n",
    "\n",
    "    def make_ds(inds):\n",
    "        x = X[inds]\n",
    "        y = {'stressed_not_stressed_output': y_binary[inds], 'emotion_class_output': y_multi_onehot[inds]}\n",
    "        sw = {'stressed_not_stressed_output': stress_sample_weights[inds], 'emotion_class_output': emotion_sample_weights[inds]}\n",
    "        return tf.data.Dataset.from_tensor_slices((x, y, sw)).shuffle(len(inds), seed=random_state).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    datasets = {'train': make_ds(train_indices), 'val': make_ds(val_indices), 'test': make_ds(test_indices)}\n",
    "    meta = {'counts': {cls: labels.count(cls) for cls in CLASS_NAMES}, 'total_samples': len(X), 'class_weights': class_weights, 'index_to_class': {i: c for i, c in enumerate(CLASS_NAMES)}}\n",
    "    return datasets, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ac15d",
   "metadata": {},
   "source": [
    "### Model (Conv1D-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2edede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. A Better Model Architecture (Conv1D-LSTM)\n",
    "# ==============================================================================\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
    "        super(Attention, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        et = tf.keras.backend.squeeze(tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b), axis=-1)\n",
    "        at = tf.keras.backend.softmax(et)\n",
    "        at = tf.keras.backend.expand_dims(at, axis=-1)\n",
    "        output = x * at\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "def create_eeg_model(input_shape):\n",
    "    \"\"\"Create a Conv1D-LSTM model designed for multi-channel time-series.\"\"\"\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1: Temporal Feature Extraction\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Block 2: Deeper Temporal Features\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Block 3: Recurrent Layers for sequence modeling\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Attention and Output heads\n",
    "    attention_output = Attention()(x)\n",
    "    main_path = GlobalAveragePooling1D()(x)\n",
    "    main_path = concatenate([main_path, attention_output])\n",
    "    \n",
    "    binary_head = Dense(32, activation='relu')(main_path)\n",
    "    binary_head = Dropout(0.5)(binary_head)\n",
    "    binary_head_output = Dense(1, activation='sigmoid', name='stressed_not_stressed_output')(binary_head)\n",
    "\n",
    "    multiclass_head = Dense(32, activation='relu')(main_path)\n",
    "    multiclass_head = Dropout(0.5)(multiclass_head)\n",
    "    multiclass_head_output = Dense(len(CLASS_NAMES), activation='softmax', name='emotion_class_output')(multiclass_head)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=input_layer,\n",
    "        outputs={\n",
    "            \"stressed_not_stressed_output\": binary_head_output,\n",
    "            \"emotion_class_output\": multiclass_head_output\n",
    "        }\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7ba8a",
   "metadata": {},
   "source": [
    "### Main Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646f412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preprocessing Dataset ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758500795.364729  109211 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1133 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building Model ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,376</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ emotion_class_outp… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ stressed_not_stres… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m14\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m1,376\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m3,104\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │      \u001b[38;5;34m6,208\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m12,352\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m66,048\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m41,216\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m320\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m4,128\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m4,128\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ emotion_class_outp… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m264\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ stressed_not_stres… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139,945</span> (546.66 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m139,945\u001b[0m (546.66 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139,561</span> (545.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m139,561\u001b[0m (545.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training ---\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 05:56:40.868555: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - emotion_class_output_accuracy: 0.1776 - emotion_class_output_loss: 2.0833 - loss: 2.4180 - stressed_not_stressed_output_accuracy: 0.6140 - stressed_not_stressed_output_loss: 0.6694\n",
      "Epoch 1: val_emotion_class_output_accuracy improved from None to 0.11111, saving model to models/best_model_20250922-055636.keras\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - emotion_class_output_accuracy: 0.1325 - emotion_class_output_loss: 2.0977 - loss: 2.4197 - stressed_not_stressed_output_accuracy: 0.6807 - stressed_not_stressed_output_loss: 0.6418 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.0814 - val_loss: 2.4368 - val_stressed_not_stressed_output_accuracy: 0.2500 - val_stressed_not_stressed_output_loss: 0.7108 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.1401 - emotion_class_output_loss: 2.1152 - loss: 2.4128 - stressed_not_stressed_output_accuracy: 0.7386 - stressed_not_stressed_output_loss: 0.5951\n",
      "Epoch 2: val_emotion_class_output_accuracy improved from 0.11111 to 0.16667, saving model to models/best_model_20250922-055636.keras\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.1325 - emotion_class_output_loss: 2.0962 - loss: 2.3893 - stressed_not_stressed_output_accuracy: 0.7470 - stressed_not_stressed_output_loss: 0.5832 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.0807 - val_loss: 2.4505 - val_stressed_not_stressed_output_accuracy: 0.2500 - val_stressed_not_stressed_output_loss: 0.7395 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.1340 - emotion_class_output_loss: 2.0790 - loss: 2.3341 - stressed_not_stressed_output_accuracy: 0.7806 - stressed_not_stressed_output_loss: 0.5103\n",
      "Epoch 3: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.1928 - emotion_class_output_loss: 2.0626 - loss: 2.3402 - stressed_not_stressed_output_accuracy: 0.7470 - stressed_not_stressed_output_loss: 0.5612 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.0939 - val_loss: 2.4734 - val_stressed_not_stressed_output_accuracy: 0.2778 - val_stressed_not_stressed_output_loss: 0.7589 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.2179 - emotion_class_output_loss: 2.0484 - loss: 2.3414 - stressed_not_stressed_output_accuracy: 0.7193 - stressed_not_stressed_output_loss: 0.5860\n",
      "Epoch 4: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.1807 - emotion_class_output_loss: 2.0683 - loss: 2.3448 - stressed_not_stressed_output_accuracy: 0.7410 - stressed_not_stressed_output_loss: 0.5502 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.1088 - val_loss: 2.5026 - val_stressed_not_stressed_output_accuracy: 0.2500 - val_stressed_not_stressed_output_loss: 0.7877 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.2045 - emotion_class_output_loss: 2.0576 - loss: 2.3383 - stressed_not_stressed_output_accuracy: 0.7362 - stressed_not_stressed_output_loss: 0.5613\n",
      "Epoch 5: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - emotion_class_output_accuracy: 0.2108 - emotion_class_output_loss: 2.0495 - loss: 2.3209 - stressed_not_stressed_output_accuracy: 0.7470 - stressed_not_stressed_output_loss: 0.5427 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.1103 - val_loss: 2.5061 - val_stressed_not_stressed_output_accuracy: 0.2500 - val_stressed_not_stressed_output_loss: 0.7916 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.2176 - emotion_class_output_loss: 2.0270 - loss: 2.2793 - stressed_not_stressed_output_accuracy: 0.7882 - stressed_not_stressed_output_loss: 0.5047\n",
      "Epoch 6: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - emotion_class_output_accuracy: 0.2169 - emotion_class_output_loss: 2.0123 - loss: 2.2716 - stressed_not_stressed_output_accuracy: 0.7651 - stressed_not_stressed_output_loss: 0.5174 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.1291 - val_loss: 2.5196 - val_stressed_not_stressed_output_accuracy: 0.3611 - val_stressed_not_stressed_output_loss: 0.7811 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.2345 - emotion_class_output_loss: 1.9993 - loss: 2.2288 - stressed_not_stressed_output_accuracy: 0.8098 - stressed_not_stressed_output_loss: 0.4589\n",
      "Epoch 7: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.2169 - emotion_class_output_loss: 1.9986 - loss: 2.2471 - stressed_not_stressed_output_accuracy: 0.7771 - stressed_not_stressed_output_loss: 0.5093 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.1495 - val_loss: 2.5447 - val_stressed_not_stressed_output_accuracy: 0.3611 - val_stressed_not_stressed_output_loss: 0.7905 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.1891 - emotion_class_output_loss: 1.9884 - loss: 2.2220 - stressed_not_stressed_output_accuracy: 0.8236 - stressed_not_stressed_output_loss: 0.4672\n",
      "Epoch 8: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - emotion_class_output_accuracy: 0.2892 - emotion_class_output_loss: 1.9676 - loss: 2.2041 - stressed_not_stressed_output_accuracy: 0.8072 - stressed_not_stressed_output_loss: 0.4775 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.1866 - val_loss: 2.6344 - val_stressed_not_stressed_output_accuracy: 0.2778 - val_stressed_not_stressed_output_loss: 0.8957 - learning_rate: 2.5000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.2590 - emotion_class_output_loss: 1.9523 - loss: 2.1830 - stressed_not_stressed_output_accuracy: 0.8113 - stressed_not_stressed_output_loss: 0.4615\n",
      "Epoch 9: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - emotion_class_output_accuracy: 0.2831 - emotion_class_output_loss: 1.9246 - loss: 2.1429 - stressed_not_stressed_output_accuracy: 0.8072 - stressed_not_stressed_output_loss: 0.4380 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.1607 - val_loss: 2.5685 - val_stressed_not_stressed_output_accuracy: 0.3611 - val_stressed_not_stressed_output_loss: 0.8155 - learning_rate: 2.5000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.2910 - emotion_class_output_loss: 1.9301 - loss: 2.1517 - stressed_not_stressed_output_accuracy: 0.8411 - stressed_not_stressed_output_loss: 0.4432\n",
      "Epoch 10: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.3072 - emotion_class_output_loss: 1.9276 - loss: 2.1585 - stressed_not_stressed_output_accuracy: 0.8313 - stressed_not_stressed_output_loss: 0.4534 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.1867 - val_loss: 2.5816 - val_stressed_not_stressed_output_accuracy: 0.4444 - val_stressed_not_stressed_output_loss: 0.7897 - learning_rate: 2.5000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.3153 - emotion_class_output_loss: 1.8512 - loss: 2.0257 - stressed_not_stressed_output_accuracy: 0.8290 - stressed_not_stressed_output_loss: 0.3491\n",
      "Epoch 11: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.3072 - emotion_class_output_loss: 1.8837 - loss: 2.0795 - stressed_not_stressed_output_accuracy: 0.8253 - stressed_not_stressed_output_loss: 0.3930 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.1555 - val_loss: 2.4765 - val_stressed_not_stressed_output_accuracy: 0.5556 - val_stressed_not_stressed_output_loss: 0.6421 - learning_rate: 2.5000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.3300 - emotion_class_output_loss: 1.8254 - loss: 2.0076 - stressed_not_stressed_output_accuracy: 0.8658 - stressed_not_stressed_output_loss: 0.3642\n",
      "Epoch 12: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.3193 - emotion_class_output_loss: 1.8291 - loss: 2.0115 - stressed_not_stressed_output_accuracy: 0.8675 - stressed_not_stressed_output_loss: 0.3597 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.2339 - val_loss: 2.6431 - val_stressed_not_stressed_output_accuracy: 0.4722 - val_stressed_not_stressed_output_loss: 0.8185 - learning_rate: 2.5000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.3016 - emotion_class_output_loss: 1.8235 - loss: 1.9842 - stressed_not_stressed_output_accuracy: 0.9033 - stressed_not_stressed_output_loss: 0.3214\n",
      "Epoch 13: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.3313 - emotion_class_output_loss: 1.7910 - loss: 1.9688 - stressed_not_stressed_output_accuracy: 0.8735 - stressed_not_stressed_output_loss: 0.3570 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.2100 - val_loss: 2.5658 - val_stressed_not_stressed_output_accuracy: 0.5278 - val_stressed_not_stressed_output_loss: 0.7117 - learning_rate: 1.2500e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.4761 - emotion_class_output_loss: 1.6609 - loss: 1.8170 - stressed_not_stressed_output_accuracy: 0.9046 - stressed_not_stressed_output_loss: 0.3122\n",
      "Epoch 14: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - emotion_class_output_accuracy: 0.4157 - emotion_class_output_loss: 1.7305 - loss: 1.8840 - stressed_not_stressed_output_accuracy: 0.9096 - stressed_not_stressed_output_loss: 0.3165 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.2429 - val_loss: 2.6330 - val_stressed_not_stressed_output_accuracy: 0.4722 - val_stressed_not_stressed_output_loss: 0.7801 - learning_rate: 1.2500e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.3340 - emotion_class_output_loss: 1.7817 - loss: 1.9175 - stressed_not_stressed_output_accuracy: 0.9258 - stressed_not_stressed_output_loss: 0.2716\n",
      "Epoch 15: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.3675 - emotion_class_output_loss: 1.7439 - loss: 1.8919 - stressed_not_stressed_output_accuracy: 0.8855 - stressed_not_stressed_output_loss: 0.2916 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.3066 - val_loss: 2.7483 - val_stressed_not_stressed_output_accuracy: 0.4444 - val_stressed_not_stressed_output_loss: 0.8835 - learning_rate: 1.2500e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4398 - emotion_class_output_loss: 1.7230 - loss: 1.8767 - stressed_not_stressed_output_accuracy: 0.8803 - stressed_not_stressed_output_loss: 0.3073\n",
      "Epoch 16: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4398 - emotion_class_output_loss: 1.6934 - loss: 1.8398 - stressed_not_stressed_output_accuracy: 0.8976 - stressed_not_stressed_output_loss: 0.2823 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.3104 - val_loss: 2.7511 - val_stressed_not_stressed_output_accuracy: 0.4444 - val_stressed_not_stressed_output_loss: 0.8814 - learning_rate: 1.2500e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.4092 - emotion_class_output_loss: 1.6071 - loss: 1.7338 - stressed_not_stressed_output_accuracy: 0.9074 - stressed_not_stressed_output_loss: 0.2534\n",
      "Epoch 17: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.3916 - emotion_class_output_loss: 1.6687 - loss: 1.7922 - stressed_not_stressed_output_accuracy: 0.9217 - stressed_not_stressed_output_loss: 0.2458 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.3172 - val_loss: 2.6987 - val_stressed_not_stressed_output_accuracy: 0.6111 - val_stressed_not_stressed_output_loss: 0.7628 - learning_rate: 1.2500e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4112 - emotion_class_output_loss: 1.6425 - loss: 1.7516 - stressed_not_stressed_output_accuracy: 0.9422 - stressed_not_stressed_output_loss: 0.2181\n",
      "Epoch 18: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4277 - emotion_class_output_loss: 1.6251 - loss: 1.7384 - stressed_not_stressed_output_accuracy: 0.9277 - stressed_not_stressed_output_loss: 0.2268 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.3904 - val_loss: 2.8465 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 0.9121 - learning_rate: 6.2500e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.3781 - emotion_class_output_loss: 1.6132 - loss: 1.7161 - stressed_not_stressed_output_accuracy: 0.9250 - stressed_not_stressed_output_loss: 0.2058\n",
      "Epoch 19: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.3795 - emotion_class_output_loss: 1.6551 - loss: 1.7555 - stressed_not_stressed_output_accuracy: 0.9337 - stressed_not_stressed_output_loss: 0.2032 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.3337 - val_loss: 2.7354 - val_stressed_not_stressed_output_accuracy: 0.5556 - val_stressed_not_stressed_output_loss: 0.8035 - learning_rate: 6.2500e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4679 - emotion_class_output_loss: 1.5362 - loss: 1.6493 - stressed_not_stressed_output_accuracy: 0.8960 - stressed_not_stressed_output_loss: 0.2261\n",
      "Epoch 20: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4217 - emotion_class_output_loss: 1.5636 - loss: 1.6664 - stressed_not_stressed_output_accuracy: 0.9157 - stressed_not_stressed_output_loss: 0.1958 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.4062 - val_loss: 2.8977 - val_stressed_not_stressed_output_accuracy: 0.4722 - val_stressed_not_stressed_output_loss: 0.9830 - learning_rate: 6.2500e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4925 - emotion_class_output_loss: 1.6089 - loss: 1.7018 - stressed_not_stressed_output_accuracy: 0.9461 - stressed_not_stressed_output_loss: 0.1856\n",
      "Epoch 21: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4458 - emotion_class_output_loss: 1.6000 - loss: 1.7022 - stressed_not_stressed_output_accuracy: 0.9277 - stressed_not_stressed_output_loss: 0.2054 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.3883 - val_loss: 2.8495 - val_stressed_not_stressed_output_accuracy: 0.5278 - val_stressed_not_stressed_output_loss: 0.9224 - learning_rate: 6.2500e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4141 - emotion_class_output_loss: 1.5364 - loss: 1.6544 - stressed_not_stressed_output_accuracy: 0.9569 - stressed_not_stressed_output_loss: 0.2360\n",
      "Epoch 22: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.3554 - emotion_class_output_loss: 1.6159 - loss: 1.7086 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1985 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.3721 - val_loss: 2.8004 - val_stressed_not_stressed_output_accuracy: 0.5278 - val_stressed_not_stressed_output_loss: 0.8567 - learning_rate: 6.2500e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4040 - emotion_class_output_loss: 1.6409 - loss: 1.7731 - stressed_not_stressed_output_accuracy: 0.9256 - stressed_not_stressed_output_loss: 0.2643\n",
      "Epoch 23: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.3675 - emotion_class_output_loss: 1.6618 - loss: 1.7795 - stressed_not_stressed_output_accuracy: 0.9337 - stressed_not_stressed_output_loss: 0.2306 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.4062 - val_loss: 2.8790 - val_stressed_not_stressed_output_accuracy: 0.4722 - val_stressed_not_stressed_output_loss: 0.9456 - learning_rate: 3.1250e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4493 - emotion_class_output_loss: 1.5250 - loss: 1.6030 - stressed_not_stressed_output_accuracy: 0.9454 - stressed_not_stressed_output_loss: 0.1561\n",
      "Epoch 24: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4337 - emotion_class_output_loss: 1.5419 - loss: 1.6258 - stressed_not_stressed_output_accuracy: 0.9337 - stressed_not_stressed_output_loss: 0.1610 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.4023 - val_loss: 2.8697 - val_stressed_not_stressed_output_accuracy: 0.5278 - val_stressed_not_stressed_output_loss: 0.9349 - learning_rate: 3.1250e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4789 - emotion_class_output_loss: 1.5179 - loss: 1.6038 - stressed_not_stressed_output_accuracy: 0.9699 - stressed_not_stressed_output_loss: 0.1718\n",
      "Epoch 25: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4518 - emotion_class_output_loss: 1.5528 - loss: 1.6378 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1815 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.4618 - val_loss: 2.9910 - val_stressed_not_stressed_output_accuracy: 0.4722 - val_stressed_not_stressed_output_loss: 1.0584 - learning_rate: 3.1250e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4618 - emotion_class_output_loss: 1.4654 - loss: 1.5726 - stressed_not_stressed_output_accuracy: 0.9237 - stressed_not_stressed_output_loss: 0.2145\n",
      "Epoch 26: val_emotion_class_output_accuracy did not improve from 0.16667\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4096 - emotion_class_output_loss: 1.5496 - loss: 1.6480 - stressed_not_stressed_output_accuracy: 0.9398 - stressed_not_stressed_output_loss: 0.1962 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.4341 - val_loss: 2.9250 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 0.9819 - learning_rate: 3.1250e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.5057 - emotion_class_output_loss: 1.5459 - loss: 1.6175 - stressed_not_stressed_output_accuracy: 0.9583 - stressed_not_stressed_output_loss: 0.1432\n",
      "Epoch 27: val_emotion_class_output_accuracy improved from 0.16667 to 0.19444, saving model to models/best_model_20250922-055636.keras\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - emotion_class_output_accuracy: 0.4880 - emotion_class_output_loss: 1.5456 - loss: 1.6168 - stressed_not_stressed_output_accuracy: 0.9518 - stressed_not_stressed_output_loss: 0.1533 - val_emotion_class_output_accuracy: 0.1944 - val_emotion_class_output_loss: 2.3691 - val_loss: 2.7848 - val_stressed_not_stressed_output_accuracy: 0.5556 - val_stressed_not_stressed_output_loss: 0.8312 - learning_rate: 3.1250e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5116 - emotion_class_output_loss: 1.4665 - loss: 1.5513 - stressed_not_stressed_output_accuracy: 0.9710 - stressed_not_stressed_output_loss: 0.1695\n",
      "Epoch 28: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4819 - emotion_class_output_loss: 1.5095 - loss: 1.5854 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1617 - val_emotion_class_output_accuracy: 0.1111 - val_emotion_class_output_loss: 2.4286 - val_loss: 2.9063 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 0.9556 - learning_rate: 3.1250e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5052 - emotion_class_output_loss: 1.4053 - loss: 1.4794 - stressed_not_stressed_output_accuracy: 0.9411 - stressed_not_stressed_output_loss: 0.1482\n",
      "Epoch 29: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4458 - emotion_class_output_loss: 1.4478 - loss: 1.5265 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1634 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.4491 - val_loss: 2.9467 - val_stressed_not_stressed_output_accuracy: 0.4722 - val_stressed_not_stressed_output_loss: 0.9952 - learning_rate: 3.1250e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4280 - emotion_class_output_loss: 1.5612 - loss: 1.6613 - stressed_not_stressed_output_accuracy: 0.9688 - stressed_not_stressed_output_loss: 0.2001\n",
      "Epoch 30: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4518 - emotion_class_output_loss: 1.5190 - loss: 1.6185 - stressed_not_stressed_output_accuracy: 0.9398 - stressed_not_stressed_output_loss: 0.2119 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.4995 - val_loss: 3.0398 - val_stressed_not_stressed_output_accuracy: 0.4722 - val_stressed_not_stressed_output_loss: 1.0806 - learning_rate: 3.1250e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4483 - emotion_class_output_loss: 1.4746 - loss: 1.5432 - stressed_not_stressed_output_accuracy: 0.9625 - stressed_not_stressed_output_loss: 0.1372\n",
      "Epoch 31: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4157 - emotion_class_output_loss: 1.5091 - loss: 1.5858 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1384 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.4348 - val_loss: 2.9009 - val_stressed_not_stressed_output_accuracy: 0.5278 - val_stressed_not_stressed_output_loss: 0.9323 - learning_rate: 3.1250e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4825 - emotion_class_output_loss: 1.4647 - loss: 1.5594 - stressed_not_stressed_output_accuracy: 0.9349 - stressed_not_stressed_output_loss: 0.1894\n",
      "Epoch 32: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4639 - emotion_class_output_loss: 1.4744 - loss: 1.5740 - stressed_not_stressed_output_accuracy: 0.9277 - stressed_not_stressed_output_loss: 0.1952 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.4612 - val_loss: 2.9531 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 0.9839 - learning_rate: 3.1250e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4593 - emotion_class_output_loss: 1.5009 - loss: 1.5634 - stressed_not_stressed_output_accuracy: 0.9665 - stressed_not_stressed_output_loss: 0.1249\n",
      "Epoch 33: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4277 - emotion_class_output_loss: 1.4937 - loss: 1.5588 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1428 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5054 - val_loss: 3.0391 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0673 - learning_rate: 1.5625e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5939 - emotion_class_output_loss: 1.4141 - loss: 1.4812 - stressed_not_stressed_output_accuracy: 0.9818 - stressed_not_stressed_output_loss: 0.1342\n",
      "Epoch 34: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.5602 - emotion_class_output_loss: 1.4262 - loss: 1.4918 - stressed_not_stressed_output_accuracy: 0.9759 - stressed_not_stressed_output_loss: 0.1367 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5070 - val_loss: 3.0336 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0531 - learning_rate: 1.5625e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4930 - emotion_class_output_loss: 1.4279 - loss: 1.4960 - stressed_not_stressed_output_accuracy: 0.9493 - stressed_not_stressed_output_loss: 0.1362\n",
      "Epoch 35: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4699 - emotion_class_output_loss: 1.4762 - loss: 1.5377 - stressed_not_stressed_output_accuracy: 0.9518 - stressed_not_stressed_output_loss: 0.1412 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5063 - val_loss: 3.0311 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0495 - learning_rate: 1.5625e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.5874 - emotion_class_output_loss: 1.3738 - loss: 1.4345 - stressed_not_stressed_output_accuracy: 0.9695 - stressed_not_stressed_output_loss: 0.1214\n",
      "Epoch 36: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - emotion_class_output_accuracy: 0.5181 - emotion_class_output_loss: 1.4398 - loss: 1.4966 - stressed_not_stressed_output_accuracy: 0.9699 - stressed_not_stressed_output_loss: 0.1155 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.4942 - val_loss: 3.0073 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0261 - learning_rate: 1.5625e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - emotion_class_output_accuracy: 0.4628 - emotion_class_output_loss: 1.4656 - loss: 1.5292 - stressed_not_stressed_output_accuracy: 0.9779 - stressed_not_stressed_output_loss: 0.1271\n",
      "Epoch 37: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4639 - emotion_class_output_loss: 1.5027 - loss: 1.5811 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1475 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5113 - val_loss: 3.0428 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0630 - learning_rate: 1.5625e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4159 - emotion_class_output_loss: 1.4949 - loss: 1.5564 - stressed_not_stressed_output_accuracy: 0.9621 - stressed_not_stressed_output_loss: 0.1230\n",
      "Epoch 38: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4337 - emotion_class_output_loss: 1.4778 - loss: 1.5450 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1395 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5244 - val_loss: 3.0639 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0790 - learning_rate: 7.8125e-06\n",
      "Epoch 39/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - emotion_class_output_accuracy: 0.4583 - emotion_class_output_loss: 1.4877 - loss: 1.5539 - stressed_not_stressed_output_accuracy: 0.9670 - stressed_not_stressed_output_loss: 0.1325\n",
      "Epoch 39: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - emotion_class_output_accuracy: 0.4880 - emotion_class_output_loss: 1.4120 - loss: 1.4792 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1490 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5199 - val_loss: 3.0540 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0681 - learning_rate: 7.8125e-06\n",
      "Epoch 40/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4643 - emotion_class_output_loss: 1.4561 - loss: 1.5231 - stressed_not_stressed_output_accuracy: 0.9610 - stressed_not_stressed_output_loss: 0.1341\n",
      "Epoch 40: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4699 - emotion_class_output_loss: 1.4039 - loss: 1.4768 - stressed_not_stressed_output_accuracy: 0.9458 - stressed_not_stressed_output_loss: 0.1430 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5174 - val_loss: 3.0479 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0609 - learning_rate: 7.8125e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.5492 - emotion_class_output_loss: 1.3595 - loss: 1.4155 - stressed_not_stressed_output_accuracy: 0.9892 - stressed_not_stressed_output_loss: 0.1121\n",
      "Epoch 41: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.5181 - emotion_class_output_loss: 1.4191 - loss: 1.4656 - stressed_not_stressed_output_accuracy: 0.9819 - stressed_not_stressed_output_loss: 0.1091 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5135 - val_loss: 3.0359 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0448 - learning_rate: 7.8125e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5367 - emotion_class_output_loss: 1.4117 - loss: 1.4668 - stressed_not_stressed_output_accuracy: 0.9830 - stressed_not_stressed_output_loss: 0.1101\n",
      "Epoch 42: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5060 - emotion_class_output_loss: 1.4478 - loss: 1.5099 - stressed_not_stressed_output_accuracy: 0.9759 - stressed_not_stressed_output_loss: 0.1126 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5189 - val_loss: 3.0464 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0550 - learning_rate: 7.8125e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4260 - emotion_class_output_loss: 1.4462 - loss: 1.5229 - stressed_not_stressed_output_accuracy: 0.9621 - stressed_not_stressed_output_loss: 0.1533\n",
      "Epoch 43: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4217 - emotion_class_output_loss: 1.4943 - loss: 1.5600 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1412 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5226 - val_loss: 3.0555 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0659 - learning_rate: 3.9063e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.3772 - emotion_class_output_loss: 1.5129 - loss: 1.5841 - stressed_not_stressed_output_accuracy: 0.9466 - stressed_not_stressed_output_loss: 0.1422\n",
      "Epoch 44: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4398 - emotion_class_output_loss: 1.4761 - loss: 1.5418 - stressed_not_stressed_output_accuracy: 0.9398 - stressed_not_stressed_output_loss: 0.1446 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5246 - val_loss: 3.0591 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0690 - learning_rate: 3.9063e-06\n",
      "Epoch 45/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5069 - emotion_class_output_loss: 1.4135 - loss: 1.4876 - stressed_not_stressed_output_accuracy: 0.9627 - stressed_not_stressed_output_loss: 0.1482\n",
      "Epoch 45: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4518 - emotion_class_output_loss: 1.4691 - loss: 1.5278 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1372 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5354 - val_loss: 3.0799 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0889 - learning_rate: 3.9063e-06\n",
      "Epoch 46/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5257 - emotion_class_output_loss: 1.4061 - loss: 1.4618 - stressed_not_stressed_output_accuracy: 0.9826 - stressed_not_stressed_output_loss: 0.1113\n",
      "Epoch 46: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5060 - emotion_class_output_loss: 1.4791 - loss: 1.5362 - stressed_not_stressed_output_accuracy: 0.9819 - stressed_not_stressed_output_loss: 0.1364 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5269 - val_loss: 3.0633 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0728 - learning_rate: 3.9063e-06\n",
      "Epoch 47/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5716 - emotion_class_output_loss: 1.3434 - loss: 1.3972 - stressed_not_stressed_output_accuracy: 0.9868 - stressed_not_stressed_output_loss: 0.1076\n",
      "Epoch 47: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5181 - emotion_class_output_loss: 1.4053 - loss: 1.4483 - stressed_not_stressed_output_accuracy: 0.9880 - stressed_not_stressed_output_loss: 0.0999 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5335 - val_loss: 3.0767 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0864 - learning_rate: 3.9063e-06\n",
      "Epoch 48/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4862 - emotion_class_output_loss: 1.4590 - loss: 1.5054 - stressed_not_stressed_output_accuracy: 0.9952 - stressed_not_stressed_output_loss: 0.0928\n",
      "Epoch 48: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4518 - emotion_class_output_loss: 1.4665 - loss: 1.5220 - stressed_not_stressed_output_accuracy: 0.9880 - stressed_not_stressed_output_loss: 0.1169 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5308 - val_loss: 3.0720 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0824 - learning_rate: 1.9531e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.4789 - emotion_class_output_loss: 1.4544 - loss: 1.5252 - stressed_not_stressed_output_accuracy: 0.9466 - stressed_not_stressed_output_loss: 0.1416\n",
      "Epoch 49: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4819 - emotion_class_output_loss: 1.4497 - loss: 1.5320 - stressed_not_stressed_output_accuracy: 0.9458 - stressed_not_stressed_output_loss: 0.1488 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5370 - val_loss: 3.0838 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0937 - learning_rate: 1.9531e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.3995 - emotion_class_output_loss: 1.4709 - loss: 1.5377 - stressed_not_stressed_output_accuracy: 0.9762 - stressed_not_stressed_output_loss: 0.1336\n",
      "Epoch 50: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4518 - emotion_class_output_loss: 1.4101 - loss: 1.4721 - stressed_not_stressed_output_accuracy: 0.9819 - stressed_not_stressed_output_loss: 0.1213 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5333 - val_loss: 3.0760 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0855 - learning_rate: 1.9531e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4559 - emotion_class_output_loss: 1.3879 - loss: 1.4419 - stressed_not_stressed_output_accuracy: 0.9932 - stressed_not_stressed_output_loss: 0.1081\n",
      "Epoch 51: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4639 - emotion_class_output_loss: 1.4066 - loss: 1.4605 - stressed_not_stressed_output_accuracy: 0.9940 - stressed_not_stressed_output_loss: 0.1061 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5324 - val_loss: 3.0736 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0824 - learning_rate: 1.9531e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4168 - emotion_class_output_loss: 1.4812 - loss: 1.5667 - stressed_not_stressed_output_accuracy: 0.9607 - stressed_not_stressed_output_loss: 0.1711\n",
      "Epoch 52: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4458 - emotion_class_output_loss: 1.4852 - loss: 1.5579 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1454 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5333 - val_loss: 3.0761 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0856 - learning_rate: 1.9531e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4448 - emotion_class_output_loss: 1.4158 - loss: 1.4976 - stressed_not_stressed_output_accuracy: 0.9551 - stressed_not_stressed_output_loss: 0.1635\n",
      "Epoch 53: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4398 - emotion_class_output_loss: 1.4596 - loss: 1.5498 - stressed_not_stressed_output_accuracy: 0.9458 - stressed_not_stressed_output_loss: 0.1756 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5359 - val_loss: 3.0799 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0880 - learning_rate: 9.7656e-07\n",
      "Epoch 54/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4832 - emotion_class_output_loss: 1.3949 - loss: 1.4524 - stressed_not_stressed_output_accuracy: 0.9754 - stressed_not_stressed_output_loss: 0.1151\n",
      "Epoch 54: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4819 - emotion_class_output_loss: 1.3761 - loss: 1.4458 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1385 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5376 - val_loss: 3.0843 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0934 - learning_rate: 9.7656e-07\n",
      "Epoch 55/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - emotion_class_output_accuracy: 0.4068 - emotion_class_output_loss: 1.4495 - loss: 1.4991 - stressed_not_stressed_output_accuracy: 0.9755 - stressed_not_stressed_output_loss: 0.0992\n",
      "Epoch 55: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - emotion_class_output_accuracy: 0.4157 - emotion_class_output_loss: 1.4472 - loss: 1.4955 - stressed_not_stressed_output_accuracy: 0.9699 - stressed_not_stressed_output_loss: 0.1037 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5325 - val_loss: 3.0745 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0839 - learning_rate: 9.7656e-07\n",
      "Epoch 56/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4219 - emotion_class_output_loss: 1.4388 - loss: 1.4967 - stressed_not_stressed_output_accuracy: 0.9628 - stressed_not_stressed_output_loss: 0.1157\n",
      "Epoch 56: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4458 - emotion_class_output_loss: 1.4286 - loss: 1.4887 - stressed_not_stressed_output_accuracy: 0.9458 - stressed_not_stressed_output_loss: 0.1484 - val_emotion_class_output_accuracy: 0.1389 - val_emotion_class_output_loss: 2.5386 - val_loss: 3.0871 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0971 - learning_rate: 9.7656e-07\n",
      "Epoch 57/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4508 - emotion_class_output_loss: 1.4677 - loss: 1.5137 - stressed_not_stressed_output_accuracy: 0.9697 - stressed_not_stressed_output_loss: 0.0920\n",
      "Epoch 57: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4819 - emotion_class_output_loss: 1.4642 - loss: 1.5289 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1349 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5320 - val_loss: 3.0732 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0824 - learning_rate: 9.7656e-07\n",
      "Epoch 58/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - emotion_class_output_accuracy: 0.4241 - emotion_class_output_loss: 1.4760 - loss: 1.5351 - stressed_not_stressed_output_accuracy: 0.9707 - stressed_not_stressed_output_loss: 0.1182\n",
      "Epoch 58: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4699 - emotion_class_output_loss: 1.4134 - loss: 1.4826 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1286 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5345 - val_loss: 3.0775 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0861 - learning_rate: 4.8828e-07\n",
      "Epoch 59/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5320 - emotion_class_output_loss: 1.4416 - loss: 1.5166 - stressed_not_stressed_output_accuracy: 0.9351 - stressed_not_stressed_output_loss: 0.1499\n",
      "Epoch 59: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4819 - emotion_class_output_loss: 1.4291 - loss: 1.4936 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1358 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5341 - val_loss: 3.0764 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0847 - learning_rate: 4.8828e-07\n",
      "Epoch 60/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4730 - emotion_class_output_loss: 1.4508 - loss: 1.4969 - stressed_not_stressed_output_accuracy: 0.9770 - stressed_not_stressed_output_loss: 0.0923\n",
      "Epoch 60: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4880 - emotion_class_output_loss: 1.4267 - loss: 1.4836 - stressed_not_stressed_output_accuracy: 0.9699 - stressed_not_stressed_output_loss: 0.1165 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5343 - val_loss: 3.0755 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0824 - learning_rate: 4.8828e-07\n",
      "Epoch 61/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4269 - emotion_class_output_loss: 1.4340 - loss: 1.5288 - stressed_not_stressed_output_accuracy: 0.9349 - stressed_not_stressed_output_loss: 0.1896\n",
      "Epoch 61: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4699 - emotion_class_output_loss: 1.4385 - loss: 1.5026 - stressed_not_stressed_output_accuracy: 0.9518 - stressed_not_stressed_output_loss: 0.1507 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5345 - val_loss: 3.0768 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0846 - learning_rate: 4.8828e-07\n",
      "Epoch 62/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4852 - emotion_class_output_loss: 1.3981 - loss: 1.4586 - stressed_not_stressed_output_accuracy: 0.9566 - stressed_not_stressed_output_loss: 0.1209\n",
      "Epoch 62: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4578 - emotion_class_output_loss: 1.4453 - loss: 1.5001 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1133 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5357 - val_loss: 3.0792 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0869 - learning_rate: 4.8828e-07\n",
      "Epoch 63/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.3994 - emotion_class_output_loss: 1.5379 - loss: 1.5817 - stressed_not_stressed_output_accuracy: 0.9838 - stressed_not_stressed_output_loss: 0.0876\n",
      "Epoch 63: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5181 - emotion_class_output_loss: 1.4203 - loss: 1.4694 - stressed_not_stressed_output_accuracy: 0.9819 - stressed_not_stressed_output_loss: 0.1073 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5376 - val_loss: 3.0830 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0908 - learning_rate: 2.4414e-07\n",
      "Epoch 64/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4491 - emotion_class_output_loss: 1.4673 - loss: 1.5265 - stressed_not_stressed_output_accuracy: 0.9642 - stressed_not_stressed_output_loss: 0.1184\n",
      "Epoch 64: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4398 - emotion_class_output_loss: 1.4717 - loss: 1.5426 - stressed_not_stressed_output_accuracy: 0.9458 - stressed_not_stressed_output_loss: 0.1418 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5340 - val_loss: 3.0760 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0840 - learning_rate: 2.4414e-07\n",
      "Epoch 65/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4095 - emotion_class_output_loss: 1.5228 - loss: 1.5791 - stressed_not_stressed_output_accuracy: 0.9576 - stressed_not_stressed_output_loss: 0.1127\n",
      "Epoch 65: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4578 - emotion_class_output_loss: 1.4713 - loss: 1.5235 - stressed_not_stressed_output_accuracy: 0.9518 - stressed_not_stressed_output_loss: 0.1105 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5361 - val_loss: 3.0799 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0875 - learning_rate: 2.4414e-07\n",
      "Epoch 66/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5040 - emotion_class_output_loss: 1.4428 - loss: 1.4979 - stressed_not_stressed_output_accuracy: 0.9758 - stressed_not_stressed_output_loss: 0.1102\n",
      "Epoch 66: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5000 - emotion_class_output_loss: 1.4053 - loss: 1.4640 - stressed_not_stressed_output_accuracy: 0.9819 - stressed_not_stressed_output_loss: 0.0998 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5335 - val_loss: 3.0760 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0849 - learning_rate: 2.4414e-07\n",
      "Epoch 67/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5545 - emotion_class_output_loss: 1.3888 - loss: 1.4542 - stressed_not_stressed_output_accuracy: 0.9400 - stressed_not_stressed_output_loss: 0.1306\n",
      "Epoch 67: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5482 - emotion_class_output_loss: 1.3748 - loss: 1.4455 - stressed_not_stressed_output_accuracy: 0.9398 - stressed_not_stressed_output_loss: 0.1340 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5355 - val_loss: 3.0797 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0884 - learning_rate: 2.4414e-07\n",
      "Epoch 68/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4987 - emotion_class_output_loss: 1.4418 - loss: 1.5216 - stressed_not_stressed_output_accuracy: 0.9594 - stressed_not_stressed_output_loss: 0.1596\n",
      "Epoch 68: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4759 - emotion_class_output_loss: 1.4646 - loss: 1.5362 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1597 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5339 - val_loss: 3.0766 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0855 - learning_rate: 1.2207e-07\n",
      "Epoch 69/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4502 - emotion_class_output_loss: 1.4851 - loss: 1.5454 - stressed_not_stressed_output_accuracy: 0.9743 - stressed_not_stressed_output_loss: 0.1206\n",
      "Epoch 69: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5060 - emotion_class_output_loss: 1.4405 - loss: 1.5111 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1440 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5327 - val_loss: 3.0738 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0821 - learning_rate: 1.2207e-07\n",
      "Epoch 70/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4982 - emotion_class_output_loss: 1.4525 - loss: 1.5473 - stressed_not_stressed_output_accuracy: 0.9415 - stressed_not_stressed_output_loss: 0.1897\n",
      "Epoch 70: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.5120 - emotion_class_output_loss: 1.4221 - loss: 1.5224 - stressed_not_stressed_output_accuracy: 0.9518 - stressed_not_stressed_output_loss: 0.2052 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5284 - val_loss: 3.0654 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0740 - learning_rate: 1.2207e-07\n",
      "Epoch 71/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4658 - emotion_class_output_loss: 1.3922 - loss: 1.4510 - stressed_not_stressed_output_accuracy: 0.9675 - stressed_not_stressed_output_loss: 0.1177\n",
      "Epoch 71: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4940 - emotion_class_output_loss: 1.3762 - loss: 1.4471 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1324 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5363 - val_loss: 3.0812 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0900 - learning_rate: 1.2207e-07\n",
      "Epoch 72/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4607 - emotion_class_output_loss: 1.3849 - loss: 1.4287 - stressed_not_stressed_output_accuracy: 0.9757 - stressed_not_stressed_output_loss: 0.0876\n",
      "Epoch 72: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4819 - emotion_class_output_loss: 1.3875 - loss: 1.4407 - stressed_not_stressed_output_accuracy: 0.9759 - stressed_not_stressed_output_loss: 0.0969 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5370 - val_loss: 3.0826 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0914 - learning_rate: 1.2207e-07\n",
      "Epoch 73/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5690 - emotion_class_output_loss: 1.4079 - loss: 1.4507 - stressed_not_stressed_output_accuracy: 0.9922 - stressed_not_stressed_output_loss: 0.0856\n",
      "Epoch 73: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5361 - emotion_class_output_loss: 1.4430 - loss: 1.4833 - stressed_not_stressed_output_accuracy: 0.9880 - stressed_not_stressed_output_loss: 0.0884 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5383 - val_loss: 3.0848 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0929 - learning_rate: 1.0000e-07\n",
      "Epoch 74/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4671 - emotion_class_output_loss: 1.4697 - loss: 1.5190 - stressed_not_stressed_output_accuracy: 0.9829 - stressed_not_stressed_output_loss: 0.0985\n",
      "Epoch 74: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5060 - emotion_class_output_loss: 1.4131 - loss: 1.4827 - stressed_not_stressed_output_accuracy: 0.9759 - stressed_not_stressed_output_loss: 0.1219 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5306 - val_loss: 3.0697 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0782 - learning_rate: 1.0000e-07\n",
      "Epoch 75/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4462 - emotion_class_output_loss: 1.4657 - loss: 1.5402 - stressed_not_stressed_output_accuracy: 0.9248 - stressed_not_stressed_output_loss: 0.1489\n",
      "Epoch 75: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4578 - emotion_class_output_loss: 1.4641 - loss: 1.5376 - stressed_not_stressed_output_accuracy: 0.9398 - stressed_not_stressed_output_loss: 0.1317 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5352 - val_loss: 3.0786 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0868 - learning_rate: 1.0000e-07\n",
      "Epoch 76/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.5250 - emotion_class_output_loss: 1.3878 - loss: 1.4369 - stressed_not_stressed_output_accuracy: 0.9769 - stressed_not_stressed_output_loss: 0.0982\n",
      "Epoch 76: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.5000 - emotion_class_output_loss: 1.4620 - loss: 1.5208 - stressed_not_stressed_output_accuracy: 0.9759 - stressed_not_stressed_output_loss: 0.1241 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5334 - val_loss: 3.0749 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0828 - learning_rate: 1.0000e-07\n",
      "Epoch 77/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5373 - emotion_class_output_loss: 1.4111 - loss: 1.4739 - stressed_not_stressed_output_accuracy: 0.9638 - stressed_not_stressed_output_loss: 0.1256\n",
      "Epoch 77: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.5241 - emotion_class_output_loss: 1.4343 - loss: 1.5084 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1480 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5334 - val_loss: 3.0749 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0831 - learning_rate: 1.0000e-07\n",
      "Epoch 78/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - emotion_class_output_accuracy: 0.4879 - emotion_class_output_loss: 1.3733 - loss: 1.4291 - stressed_not_stressed_output_accuracy: 0.9979 - stressed_not_stressed_output_loss: 0.1116\n",
      "Epoch 78: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - emotion_class_output_accuracy: 0.4940 - emotion_class_output_loss: 1.4170 - loss: 1.4700 - stressed_not_stressed_output_accuracy: 0.9940 - stressed_not_stressed_output_loss: 0.1125 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5340 - val_loss: 3.0767 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0854 - learning_rate: 1.0000e-07\n",
      "Epoch 79/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4751 - emotion_class_output_loss: 1.4699 - loss: 1.5215 - stressed_not_stressed_output_accuracy: 0.9807 - stressed_not_stressed_output_loss: 0.1032\n",
      "Epoch 79: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4940 - emotion_class_output_loss: 1.4551 - loss: 1.5195 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1301 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5385 - val_loss: 3.0849 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0928 - learning_rate: 1.0000e-07\n",
      "Epoch 80/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5090 - emotion_class_output_loss: 1.3653 - loss: 1.4170 - stressed_not_stressed_output_accuracy: 0.9697 - stressed_not_stressed_output_loss: 0.1034\n",
      "Epoch 80: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4880 - emotion_class_output_loss: 1.4152 - loss: 1.4760 - stressed_not_stressed_output_accuracy: 0.9699 - stressed_not_stressed_output_loss: 0.1187 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5353 - val_loss: 3.0784 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0861 - learning_rate: 1.0000e-07\n",
      "Epoch 81/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5053 - emotion_class_output_loss: 1.4261 - loss: 1.4875 - stressed_not_stressed_output_accuracy: 0.9533 - stressed_not_stressed_output_loss: 0.1228\n",
      "Epoch 81: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4940 - emotion_class_output_loss: 1.4060 - loss: 1.4817 - stressed_not_stressed_output_accuracy: 0.9518 - stressed_not_stressed_output_loss: 0.1379 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5335 - val_loss: 3.0751 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0833 - learning_rate: 1.0000e-07\n",
      "Epoch 82/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4391 - emotion_class_output_loss: 1.4898 - loss: 1.5577 - stressed_not_stressed_output_accuracy: 0.9465 - stressed_not_stressed_output_loss: 0.1358\n",
      "Epoch 82: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4699 - emotion_class_output_loss: 1.4371 - loss: 1.4980 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1235 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5299 - val_loss: 3.0665 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0733 - learning_rate: 1.0000e-07\n",
      "Epoch 83/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4965 - emotion_class_output_loss: 1.3448 - loss: 1.4224 - stressed_not_stressed_output_accuracy: 0.9519 - stressed_not_stressed_output_loss: 0.1552\n",
      "Epoch 83: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.5301 - emotion_class_output_loss: 1.3666 - loss: 1.4250 - stressed_not_stressed_output_accuracy: 0.9759 - stressed_not_stressed_output_loss: 0.1189 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5356 - val_loss: 3.0783 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0855 - learning_rate: 1.0000e-07\n",
      "Epoch 84/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5170 - emotion_class_output_loss: 1.3098 - loss: 1.3598 - stressed_not_stressed_output_accuracy: 0.9812 - stressed_not_stressed_output_loss: 0.1000\n",
      "Epoch 84: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5060 - emotion_class_output_loss: 1.3909 - loss: 1.4442 - stressed_not_stressed_output_accuracy: 0.9880 - stressed_not_stressed_output_loss: 0.1114 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5355 - val_loss: 3.0781 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0852 - learning_rate: 1.0000e-07\n",
      "Epoch 85/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4231 - emotion_class_output_loss: 1.4369 - loss: 1.4905 - stressed_not_stressed_output_accuracy: 0.9802 - stressed_not_stressed_output_loss: 0.1072\n",
      "Epoch 85: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - emotion_class_output_accuracy: 0.4759 - emotion_class_output_loss: 1.4122 - loss: 1.4730 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1358 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5309 - val_loss: 3.0702 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0786 - learning_rate: 1.0000e-07\n",
      "Epoch 86/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5087 - emotion_class_output_loss: 1.3866 - loss: 1.4447 - stressed_not_stressed_output_accuracy: 0.9825 - stressed_not_stressed_output_loss: 0.1162\n",
      "Epoch 86: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4759 - emotion_class_output_loss: 1.4197 - loss: 1.4715 - stressed_not_stressed_output_accuracy: 0.9759 - stressed_not_stressed_output_loss: 0.1100 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5336 - val_loss: 3.0759 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0845 - learning_rate: 1.0000e-07\n",
      "Epoch 87/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4522 - emotion_class_output_loss: 1.4692 - loss: 1.5349 - stressed_not_stressed_output_accuracy: 0.9632 - stressed_not_stressed_output_loss: 0.1314\n",
      "Epoch 87: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4759 - emotion_class_output_loss: 1.4588 - loss: 1.5269 - stressed_not_stressed_output_accuracy: 0.9518 - stressed_not_stressed_output_loss: 0.1327 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5358 - val_loss: 3.0802 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0888 - learning_rate: 1.0000e-07\n",
      "Epoch 88/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4878 - emotion_class_output_loss: 1.3935 - loss: 1.4589 - stressed_not_stressed_output_accuracy: 0.9689 - stressed_not_stressed_output_loss: 0.1308\n",
      "Epoch 88: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.5060 - emotion_class_output_loss: 1.3972 - loss: 1.4578 - stressed_not_stressed_output_accuracy: 0.9759 - stressed_not_stressed_output_loss: 0.1143 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5375 - val_loss: 3.0833 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0916 - learning_rate: 1.0000e-07\n",
      "Epoch 89/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4814 - emotion_class_output_loss: 1.3990 - loss: 1.4454 - stressed_not_stressed_output_accuracy: 0.9946 - stressed_not_stressed_output_loss: 0.0928\n",
      "Epoch 89: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4819 - emotion_class_output_loss: 1.4651 - loss: 1.5250 - stressed_not_stressed_output_accuracy: 0.9819 - stressed_not_stressed_output_loss: 0.1247 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5363 - val_loss: 3.0811 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0896 - learning_rate: 1.0000e-07\n",
      "Epoch 90/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.5174 - emotion_class_output_loss: 1.4281 - loss: 1.5070 - stressed_not_stressed_output_accuracy: 0.9473 - stressed_not_stressed_output_loss: 0.1579\n",
      "Epoch 90: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4819 - emotion_class_output_loss: 1.4446 - loss: 1.5067 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1197 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5369 - val_loss: 3.0812 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0886 - learning_rate: 1.0000e-07\n",
      "Epoch 91/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4312 - emotion_class_output_loss: 1.5007 - loss: 1.5652 - stressed_not_stressed_output_accuracy: 0.9740 - stressed_not_stressed_output_loss: 0.1290\n",
      "Epoch 91: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4639 - emotion_class_output_loss: 1.4798 - loss: 1.5555 - stressed_not_stressed_output_accuracy: 0.9639 - stressed_not_stressed_output_loss: 0.1465 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5346 - val_loss: 3.0771 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0851 - learning_rate: 1.0000e-07\n",
      "Epoch 92/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4053 - emotion_class_output_loss: 1.4514 - loss: 1.5115 - stressed_not_stressed_output_accuracy: 0.9780 - stressed_not_stressed_output_loss: 0.1201\n",
      "Epoch 92: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4096 - emotion_class_output_loss: 1.4813 - loss: 1.5362 - stressed_not_stressed_output_accuracy: 0.9699 - stressed_not_stressed_output_loss: 0.1201 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5370 - val_loss: 3.0824 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0909 - learning_rate: 1.0000e-07\n",
      "Epoch 93/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4633 - emotion_class_output_loss: 1.4108 - loss: 1.4675 - stressed_not_stressed_output_accuracy: 0.9766 - stressed_not_stressed_output_loss: 0.1135\n",
      "Epoch 93: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4880 - emotion_class_output_loss: 1.3905 - loss: 1.4376 - stressed_not_stressed_output_accuracy: 0.9819 - stressed_not_stressed_output_loss: 0.0938 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5394 - val_loss: 3.0869 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0949 - learning_rate: 1.0000e-07\n",
      "Epoch 94/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.5215 - emotion_class_output_loss: 1.3748 - loss: 1.4369 - stressed_not_stressed_output_accuracy: 0.9582 - stressed_not_stressed_output_loss: 0.1241\n",
      "Epoch 94: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4759 - emotion_class_output_loss: 1.4509 - loss: 1.5057 - stressed_not_stressed_output_accuracy: 0.9699 - stressed_not_stressed_output_loss: 0.1123 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5391 - val_loss: 3.0866 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0951 - learning_rate: 1.0000e-07\n",
      "Epoch 95/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4445 - emotion_class_output_loss: 1.4121 - loss: 1.4731 - stressed_not_stressed_output_accuracy: 0.9750 - stressed_not_stressed_output_loss: 0.1220\n",
      "Epoch 95: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4819 - emotion_class_output_loss: 1.3984 - loss: 1.4562 - stressed_not_stressed_output_accuracy: 0.9699 - stressed_not_stressed_output_loss: 0.1148 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5368 - val_loss: 3.0828 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0920 - learning_rate: 1.0000e-07\n",
      "Epoch 96/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.5519 - emotion_class_output_loss: 1.4019 - loss: 1.4976 - stressed_not_stressed_output_accuracy: 0.9045 - stressed_not_stressed_output_loss: 0.1913\n",
      "Epoch 96: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.5120 - emotion_class_output_loss: 1.4138 - loss: 1.4837 - stressed_not_stressed_output_accuracy: 0.9398 - stressed_not_stressed_output_loss: 0.1418 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5428 - val_loss: 3.0930 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.1005 - learning_rate: 1.0000e-07\n",
      "Epoch 97/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - emotion_class_output_accuracy: 0.4969 - emotion_class_output_loss: 1.4274 - loss: 1.5005 - stressed_not_stressed_output_accuracy: 0.9492 - stressed_not_stressed_output_loss: 0.1462\n",
      "Epoch 97: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4759 - emotion_class_output_loss: 1.4384 - loss: 1.5036 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1390 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5409 - val_loss: 3.0890 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0963 - learning_rate: 1.0000e-07\n",
      "Epoch 98/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4851 - emotion_class_output_loss: 1.3987 - loss: 1.4674 - stressed_not_stressed_output_accuracy: 0.9554 - stressed_not_stressed_output_loss: 0.1374\n",
      "Epoch 98: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - emotion_class_output_accuracy: 0.4699 - emotion_class_output_loss: 1.4469 - loss: 1.5156 - stressed_not_stressed_output_accuracy: 0.9518 - stressed_not_stressed_output_loss: 0.1504 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5410 - val_loss: 3.0890 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0960 - learning_rate: 1.0000e-07\n",
      "Epoch 99/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - emotion_class_output_accuracy: 0.4674 - emotion_class_output_loss: 1.4726 - loss: 1.5337 - stressed_not_stressed_output_accuracy: 0.9687 - stressed_not_stressed_output_loss: 0.1221\n",
      "Epoch 99: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - emotion_class_output_accuracy: 0.4759 - emotion_class_output_loss: 1.4697 - loss: 1.5325 - stressed_not_stressed_output_accuracy: 0.9578 - stressed_not_stressed_output_loss: 0.1425 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5391 - val_loss: 3.0857 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0933 - learning_rate: 1.0000e-07\n",
      "Epoch 100/100\n",
      "\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - emotion_class_output_accuracy: 0.4700 - emotion_class_output_loss: 1.3963 - loss: 1.4468 - stressed_not_stressed_output_accuracy: 0.9808 - stressed_not_stressed_output_loss: 0.1010\n",
      "Epoch 100: val_emotion_class_output_accuracy did not improve from 0.19444\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - emotion_class_output_accuracy: 0.4759 - emotion_class_output_loss: 1.4186 - loss: 1.4630 - stressed_not_stressed_output_accuracy: 0.9699 - stressed_not_stressed_output_loss: 0.1079 - val_emotion_class_output_accuracy: 0.1667 - val_emotion_class_output_loss: 2.5408 - val_loss: 3.0886 - val_stressed_not_stressed_output_accuracy: 0.5000 - val_stressed_not_stressed_output_loss: 1.0955 - learning_rate: 1.0000e-07\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "\n",
      "--- Evaluating Model on Test Set ---\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - emotion_class_output_accuracy: 0.1944 - emotion_class_output_loss: 2.4428 - loss: 3.0300 - stressed_not_stressed_output_accuracy: 0.3611 - stressed_not_stressed_output_loss: 1.1743\n",
      "\n",
      "Test Results:\n",
      "  loss: 3.0300\n",
      "  compile_metrics: 1.1743\n",
      "  stressed_not_stressed_output_loss: 2.4428\n",
      "  emotion_class_output_loss: 0.1944\n",
      "\n",
      "Final model and metadata saved with timestamp: 20250922-055636\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 6. Main Execution Block\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- Path and Model Parameters ---\n",
    "    dataset_path = \"/media/kd/New Volume/Github/EEG-Emotion-Detection/dataset\"\n",
    "    \n",
    "    INPUT_TIME_STEPS = 1024 \n",
    "    INPUT_CHANNELS = len(EEG_CHANNELS)\n",
    "    INPUT_2D_SHAPE = (INPUT_TIME_STEPS, INPUT_CHANNELS)\n",
    "    BATCH_SIZE = 4 \n",
    "\n",
    "    # --- Load Data ---\n",
    "    print(\"--- Loading and Preprocessing Dataset ---\")\n",
    "    datasets, meta = load_eeg_dataset(\n",
    "        data_dir=dataset_path,\n",
    "        time_steps=INPUT_TIME_STEPS,\n",
    "        channels=INPUT_CHANNELS,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # --- Build and Compile Model ---\n",
    "    print(\"\\n--- Building Model ---\")\n",
    "    model = create_eeg_model(INPUT_2D_SHAPE)\n",
    "    optimizer = Adam(learning_rate=5e-4)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss={'stressed_not_stressed_output': 'binary_crossentropy', 'emotion_class_output': 'categorical_crossentropy'},\n",
    "        loss_weights={'stressed_not_stressed_output': 0.5, 'emotion_class_output': 1.0},\n",
    "        metrics={'stressed_not_stressed_output': 'accuracy', 'emotion_class_output': 'accuracy'}\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    # --- Define Callbacks ---\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'models/best_model_{timestamp}.keras',\n",
    "            monitor='val_emotion_class_output_accuracy', \n",
    "            save_best_only=True, \n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_emotion_class_output_accuracy', \n",
    "            patience=100, \n",
    "            restore_best_weights=True, \n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_emotion_class_output_accuracy', \n",
    "            factor=0.5, \n",
    "            patience=5, \n",
    "            min_lr=1e-7, \n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        TensorBoard(log_dir=f'logs/fit/{timestamp}', histogram_freq=1)\n",
    "    ]\n",
    "\n",
    "    # --- Train the Model ---\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    history = model.fit(\n",
    "        datasets['train'],\n",
    "        validation_data=datasets['val'],\n",
    "        epochs=100,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # --- Evaluate and Save ---\n",
    "    print(\"\\n--- Evaluating Model on Test Set ---\")\n",
    "    if datasets['test']:\n",
    "        results = model.evaluate(datasets['test'], verbose=1)\n",
    "        print(\"\\nTest Results:\")\n",
    "        for name, value in zip(model.metrics_names, results):\n",
    "            print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "    model.save(f'models/final_model_{timestamp}.keras')\n",
    "    with open(f'models/model_metadata_{timestamp}.json', 'w') as f:\n",
    "        json.dump(meta, f, indent=4)\n",
    "    print(f\"\\nFinal model and metadata saved with timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9acac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
